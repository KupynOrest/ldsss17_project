{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from extractor import get_features\n",
    "# from extractor\n",
    "os.listdir()\n",
    "\n",
    "data_dir = 'data'\n",
    "train_np_dir = 'train_np_med'\n",
    "test_np_dir = 'test_np_med'\n",
    "trained_weights_dir = 'trained_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 101\n"
     ]
    }
   ],
   "source": [
    "# Load existing clasess\n",
    "pkl_file = open('classes_dict.pickle', 'rb')\n",
    "\n",
    "classes_dict= pickle.load(pkl_file)\n",
    "\n",
    "pkl_file.close()\n",
    "\n",
    "print(classes_dict['PlayingGuitar'], len(classes_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "input_size = 512\n",
    "num_layers = 1\n",
    "dropout = 0.75\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0004\n",
    "weight_decay = 0.00005\n",
    "lr_decay_epoch = 5\n",
    "num_classes = len(classes_dict)\n",
    "sequence_length = 50\n",
    "# label_str_to_int = {'ApplyEyeMakeup': 0, 'Archery': 1, 'ApplyLipstick': 2}\n",
    "\n",
    "#Very best params (72.85% on np_big):\n",
    "# hidden_size = 512\n",
    "# input_size = 512\n",
    "# num_layers = 1\n",
    "# dropout = 0.8\n",
    "# batch_size = 128\n",
    "# num_epochs = 200\n",
    "# learning_rate = 0.0004\n",
    "# weight_decay = 0.00005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN (\n",
       "  (lstm): LSTM(512, 512, batch_first=True, dropout=0.75)\n",
       "  (fc): Linear (512 -> 101)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN Model (Many-to-One)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda()) \n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda())\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "rnn = RNN(input_size=512, hidden_size=hidden_size,\n",
    "          num_layers=num_layers, num_classes=num_classes, dropout=dropout).cuda()\n",
    "rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLSTMBatchNorm (\n",
       "  (lstm): LSTM (\n",
       "    (cell_0): BNLSTMCell (\n",
       "      (bn_ih): SeparatedBatchNorm1d(2048, eps=1e-05, momentum=0.1, max_length=512, affine=True)\n",
       "      (bn_hh): SeparatedBatchNorm1d(2048, eps=1e-05, momentum=0.1, max_length=512, affine=True)\n",
       "      (bn_c): SeparatedBatchNorm1d(512, eps=1e-05, momentum=0.1, max_length=512, affine=True)\n",
       "    )\n",
       "    (dropout_layer): Dropout (p = 0.75)\n",
       "  )\n",
       "  (fc): Linear (512 -> 101)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bnlstm import LSTM, LSTMCell, BNLSTMCell\n",
    "# RNN Model (Many-to-One)\n",
    "class ConvLSTMBatchNorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(ConvLSTMBatchNorm, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = LSTM(cell_class=BNLSTMCell, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout, max_length=input_size).cuda()\n",
    "        self.fc = nn.Linear(hidden_size, num_classes).cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = Variable(torch.zeros(x.size(0), self.hidden_size).cuda())\n",
    "        c0 = Variable(torch.zeros(x.size(0), self.hidden_size).cuda())\n",
    "        \n",
    "        _, (out, _) = self.lstm(input_=x, hx=(h0, c0))\n",
    "                                  \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[0])\n",
    "        return out\n",
    "\n",
    "rnn_batch = ConvLSTMBatchNorm(input_size=512, hidden_size=hidden_size,\n",
    "          num_layers=num_layers, num_classes=num_classes, dropout=dropout).cuda()\n",
    "rnn_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available:  True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print (\"GPU is available: \", use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn_batch.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=learning_rate, lr_decay_epoch=lr_decay_epoch):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.5**(epoch // lr_decay_epoch))\n",
    "#     lr = init_lr #* 1 / (1 + )\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copute_dataset_size(path=''):\n",
    "    data_classes = [i for i in os.listdir(path) if not i.startswith('.')]\n",
    "    num_entries = 0\n",
    "\n",
    "    for folder in data_classes:\n",
    "        current_dir = path + '/' + folder + '/'\n",
    "        num_entries += len(os.listdir(current_dir))\n",
    "        \n",
    "    return num_entries\n",
    "\n",
    "train_size = copute_dataset_size(path=data_dir + '/' + train_np_dir)\n",
    "test_size = copute_dataset_size(path=data_dir + '/' + test_np_dir)\n",
    "\n",
    "def check_size(address, sequence_size=50):\n",
    "    x = np.load(address)\n",
    "    print (x.shape)\n",
    "    if x.shape[0] < sequence_size:\n",
    "        return np.concatenate((x, np.zeros((sequence_size - x.shape[0], x.shape[1]))), axis=0)\n",
    "    return x\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "def get_loader(path='', batch_size=batch_size):\n",
    "    \"\"\"\n",
    "    Function reads .npy files from path.\n",
    "    Returns:\n",
    "        dataloader, data classes (list), size of input object [n_sequence, n_features], lenght_of_dataset\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    data_classes = [i for i in os.listdir(path) if not i.startswith('.')]\n",
    "\n",
    "    for folder in data_classes:\n",
    "        current_dir = path + '/' + folder + '/'\n",
    "        files = os.listdir(current_dir)\n",
    "        #test_f = np.load(current_dir + files[0])[:,:30,:]\n",
    "        \n",
    "#         print(test_f.shape)\n",
    "        temp = [torch.Tensor(np.load(current_dir +  f).reshape((sequence_length, input_size))) for f in files] \n",
    "                         # Transform to torch tensors\n",
    "        \n",
    "        targets += ([torch.LongTensor([classes_dict[folder]])] * len(temp))\n",
    "        inputs += temp\n",
    "\n",
    "    tensor_x = torch.stack(inputs)\n",
    "    tensor_y = torch.stack(targets)\n",
    "    my_dataset = torch.utils.data.TensorDataset(tensor_x, tensor_y)  # Create your datset\n",
    "    my_dataloader = torch.utils.data.DataLoader(my_dataset, batch_size=batch_size, shuffle=True)  # Create your dataloader\n",
    "\n",
    "    return (my_dataloader, data_classes)\n",
    "\n",
    "dset_loaders = {x: get_loader(data_dir + '/' + x)[0] for x in [train_np_dir, test_np_dir]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.listdir(data_dir + '/' + test_np_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_np_med': 3783, 'train_np_med': 9522}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset_sizes = {train_np_dir: train_size, test_np_dir: test_size}\n",
    "dset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, lr_scheduler, path, num_epochs=200, model_name='lstm'):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    training_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [train_np_dir, test_np_dir]:\n",
    "            if phase == train_np_dir:\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            counter = 0\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                # get the inputs\n",
    "\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "#                     print(inputs)\n",
    "                    inputs = Variable(inputs.view(-1, sequence_length, input_size).cuda())\n",
    "                    labels = Variable(labels.view(-1).cuda())                        \n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                    \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "#                 print (inputs.size())\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "#                 print (labels, outputs)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == train_np_dir:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             print(preds, labels.data, '\\n========')\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dset_sizes[phase]\n",
    "\n",
    "            print('running corrects: ', running_corrects)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == train_np_dir:\n",
    "                training_acc = epoch_acc\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == test_np_dir and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        # saving weights\n",
    "        torch.save(model, data_dir + '/' + trained_weights_dir + \"/\" + model_name + '_' + str(epoch) + \".pt\")\n",
    "        if (training_acc == 1):\n",
    "            print ('accuracy 1 reached, stopping...', training_acc)\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "LR is set to 0.0004\n",
      "running corrects:  3523\n",
      "train_np_med Loss: 0.0341 Acc: 0.3700\n",
      "running corrects:  1724\n",
      "test_np_med Loss: 0.0315 Acc: 0.4557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philip/.virtualenvs/cv3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type ConvLSTMBatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/199\n",
      "----------\n",
      "running corrects:  5750\n",
      "train_np_med Loss: 0.0275 Acc: 0.6039\n",
      "running corrects:  2024\n",
      "test_np_med Loss: 0.0234 Acc: 0.5350\n",
      "Epoch 2/199\n",
      "----------\n",
      "running corrects:  6909\n",
      "train_np_med Loss: 0.0182 Acc: 0.7256\n",
      "running corrects:  2260\n",
      "test_np_med Loss: 0.0165 Acc: 0.5974\n",
      "Epoch 3/199\n",
      "----------\n",
      "running corrects:  8023\n",
      "train_np_med Loss: 0.0102 Acc: 0.8426\n",
      "running corrects:  2466\n",
      "test_np_med Loss: 0.0131 Acc: 0.6519\n",
      "Epoch 4/199\n",
      "----------\n",
      "running corrects:  8833\n",
      "train_np_med Loss: 0.0053 Acc: 0.9276\n",
      "running corrects:  2560\n",
      "test_np_med Loss: 0.0112 Acc: 0.6767\n",
      "Epoch 5/199\n",
      "----------\n",
      "LR is set to 0.0002\n",
      "running corrects:  9271\n",
      "train_np_med Loss: 0.0027 Acc: 0.9736\n",
      "running corrects:  2663\n",
      "test_np_med Loss: 0.0100 Acc: 0.7039\n",
      "Epoch 6/199\n",
      "----------\n",
      "running corrects:  9401\n",
      "train_np_med Loss: 0.0017 Acc: 0.9873\n",
      "running corrects:  2708\n",
      "test_np_med Loss: 0.0099 Acc: 0.7158\n",
      "Epoch 7/199\n",
      "----------\n",
      "running corrects:  9451\n",
      "train_np_med Loss: 0.0012 Acc: 0.9925\n",
      "running corrects:  2688\n",
      "test_np_med Loss: 0.0095 Acc: 0.7105\n",
      "Epoch 8/199\n",
      "----------\n",
      "running corrects:  9477\n",
      "train_np_med Loss: 0.0009 Acc: 0.9953\n",
      "running corrects:  2680\n",
      "test_np_med Loss: 0.0098 Acc: 0.7084\n",
      "Epoch 9/199\n",
      "----------\n",
      "running corrects:  9470\n",
      "train_np_med Loss: 0.0008 Acc: 0.9945\n",
      "running corrects:  2694\n",
      "test_np_med Loss: 0.0097 Acc: 0.7121\n",
      "Epoch 10/199\n",
      "----------\n",
      "LR is set to 0.0001\n",
      "running corrects:  9507\n",
      "train_np_med Loss: 0.0006 Acc: 0.9984\n",
      "running corrects:  2717\n",
      "test_np_med Loss: 0.0091 Acc: 0.7182\n",
      "Epoch 11/199\n",
      "----------\n",
      "running corrects:  9515\n",
      "train_np_med Loss: 0.0004 Acc: 0.9993\n",
      "running corrects:  2751\n",
      "test_np_med Loss: 0.0090 Acc: 0.7272\n",
      "Epoch 12/199\n",
      "----------\n",
      "running corrects:  9518\n",
      "train_np_med Loss: 0.0004 Acc: 0.9996\n",
      "running corrects:  2723\n",
      "test_np_med Loss: 0.0091 Acc: 0.7198\n",
      "Epoch 13/199\n",
      "----------\n",
      "running corrects:  9521\n",
      "train_np_med Loss: 0.0003 Acc: 0.9999\n",
      "running corrects:  2719\n",
      "test_np_med Loss: 0.0090 Acc: 0.7187\n",
      "Epoch 14/199\n",
      "----------\n",
      "running corrects:  9522\n",
      "train_np_med Loss: 0.0003 Acc: 1.0000\n",
      "running corrects:  2733\n",
      "test_np_med Loss: 0.0089 Acc: 0.7224\n",
      "accuracy 1 reached, stopping... 1.0\n",
      "Training complete in 1m 44s\n",
      "Best val Acc: 0.727201\n"
     ]
    }
   ],
   "source": [
    "model_lstm = train_model(rnn_batch, criterion, optimizer, exp_lr_scheduler, '', num_epochs=num_epochs, model_name='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in get_loader(data_dir + '/' + test_np_dir, batch_size=3000)[0]:\n",
    "    # get the inputs\n",
    "\n",
    "    inputs, labels = data\n",
    "\n",
    "    # wrap them in Variable\n",
    "    if use_gpu:\n",
    "        x_test = Variable(inputs.view(-1, sequence_length, input_size).cuda())\n",
    "        y_test = labels.view(-1).cpu().numpy()\n",
    "        y_pred = model_lstm(x_test)\n",
    "        _, y_pred = torch.max(y_pred.data, 1)\n",
    "        y_pred = y_pred.cpu().view(-1).numpy()\n",
    "    break\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    print([round(cm[i][i], 2) for i in range(0, num_classes)])\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "# plt.figure()\n",
    "class_int_to_name = {classes_dict[key]:key for key in classes_dict}\n",
    "names_of_classes = [class_int_to_name[i] for i in range(0, num_classes)]\n",
    "print(len(names_of_classes), len(y_test))\n",
    "# plot_confusion_matrix(cnf_matrix, classes=names_of_classes,\n",
    "#                       title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=names_of_classes, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class_int_to_name = {classes_dict[key]:key for key in classes_dict}\n",
    "# class_int_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Test set is not implemented yet\n",
    "\n",
    "# # Test the Model\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images, labels in test_loader:\n",
    "#     images = Variable(images.view(-1, sequence_length, input_size))\n",
    "#     outputs = rnn(images)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     total += labels.size(0)\n",
    "#     correct += (predicted == labels).sum()\n",
    "\n",
    "# print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n",
    "\n",
    "# # Save the Model\n",
    "# # torch.save(rnn.state_dict(), 'rnn.pkl')\n",
    "# # torch.save(rnn, 'rnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dummy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_feat = 100\n",
    "# n_seq = 50 # fixed for now\n",
    "# n_video = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(4,5):\n",
    "#     tmp_data = np.random.random((n_seq+1, n_feat))\n",
    "#     np.save('data/features/Archery/v' + str(i) + '.npy', tmp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
